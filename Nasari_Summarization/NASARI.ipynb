{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc20e482-4a22-4d78-84a4-58a62bfabfc9",
   "metadata": {},
   "source": [
    "## Automatic Summarisation using Nasari\n",
    "### Consegna 3 TLN \n",
    "#### Mario Scapellato"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22bad24f",
   "metadata": {},
   "source": [
    "La risorsa NASARI rappresenta un insieme di vettori che invece di essere dei word embeddings sono dei sense embeddings in cui, i vettori che descrivono la risorsa, descrivono dei sensi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "6485cd96-b8a1-45b5-bb0d-ac4228a302d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "91b5f19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NASARI = 'utils/NASARI_vectors/dd-small-nasari-15.txt'\n",
    "NASARI_PATH = 'utils/NASARI_vectors/dd-nasari-link_210419.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4546dcb-58e2-4b98-bad9-4dce0a01276f",
   "metadata": {},
   "source": [
    "Reads the NASARI file and calculate a dict as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a5e56a58-c1a5-46c0-90b1-0f8b173dbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nasari(file):\n",
    "    nasari = {}\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            line_splitted = row.split(\";\")\n",
    "            dict_entry = {}\n",
    "            \n",
    "            # Start from 2 letter (delete \"bn:\")\n",
    "            for term in line_splitted[2:]:\n",
    "                # term and score written like this: \"serotonin_1841.0\"\n",
    "                term_score = term.split(\"_\")\n",
    "                if len(term_score) > 1:\n",
    "                    dict_entry[term_score[0]] = term_score[1]\n",
    "\n",
    "\n",
    "            #Ottengo un dizionario NASARI dalla forma  {term: score, term: score, ...}    \n",
    "            nasari[line_splitted[1].lower()] = dict_entry\n",
    "\n",
    "    return nasari"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5524a05-467b-41fb-be83-60af0aee0b21",
   "metadata": {},
   "source": [
    "Reads the given documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f4f42c7b-d0cb-413a-95f2-804b4bdc3af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(file):\n",
    "    document = []\n",
    "    with open(file, 'r', encoding=\"utf8\") as file:\n",
    "        for row in file.readlines():\n",
    "            # does not consider lines starting with \"#\"\n",
    "            if '#' not in row:\n",
    "                row = row[:-1]\n",
    "                if row != '':\n",
    "                    document.append(row)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c5eb3-132b-4f8e-90fe-aad27ba121ac",
   "metadata": {},
   "source": [
    "Computes the rank of the given vector. Method used to calculate the weighted overlap between nasari vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ddd63962-b056-40b1-8e60-6ae3a5a3d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo il rango della feature condivisa da entrambi i vettori\n",
    "def calculate_rank(vector, nasari_vector):\n",
    "    for i in range(len(nasari_vector)):\n",
    "        if nasari_vector[i] == vector:\n",
    "            # returns index of nasari_vector egual to input vector\n",
    "            return i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74a2b3-11d6-4e0d-b9b3-e867c8da0ffc",
   "metadata": {},
   "source": [
    "Implementation of the Weighted Overlap between two nasari vectors.\n",
    "$$\n",
    "  WO(w_1,w_2) = \\frac{\\sum_{q \\in O} (rank(q, v1) + rank(q, v2))^{-1}}{\\sum_{i=1}^{|O|} (2i)^{-1}}\n",
    "$$\n",
    "\n",
    "More is WO and more will be similar that 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d7d5270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo l'overlap tra due vettori NASARI\n",
    "def overlap(nasari_vector_1, nasari_vector_2):\n",
    "    overlap_keys = nasari_vector_1.keys() & nasari_vector_2.keys()\n",
    "    list_overlap_keys = list(overlap_keys)\n",
    "\n",
    "    if len(overlap_keys) != 0:\n",
    "        rank_acc =(sum( 1/ (calculate_rank (vector, list(nasari_vector_1)) + calculate_rank (vector, list(nasari_vector_2))) for vector in list_overlap_keys))\n",
    "\n",
    "        #Calcolo il denominatore\n",
    "        den = (sum(list(map(lambda x: 1 / (2 * x), list(range(1, len(list_overlap_keys) + 1))))))\n",
    "        \n",
    "        return rank_acc/den\n",
    "    \n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fea36-6b13-4650-9890-997e93f9ad28",
   "metadata": {},
   "source": [
    "A bag of word algorithm based approach. It calculates a list of word given a text doing stop word and punctuation removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "97946242-12b9-4e0f-a4dd-6698e94c9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_word_approach(text):\n",
    "    \"\"\"\n",
    "    :param text: input text\n",
    "    :return: BoW representation of the text.\n",
    "    \"\"\"\n",
    "\n",
    "    #prendo il testo, lo trasformo in minuscolo, lo tokenizzo e rimuovo le stopwords e la punteggiatura\n",
    "    text = text.lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wordnet_lemmatizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    # text tokenizzation\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # remove stop_word and punctuation\n",
    "    tokens = list(filter(lambda x: x not in stop_words and x not in string.punctuation, tokens))\n",
    "    \n",
    "    return set(wordnet_lemmatizer.lemmatize(token) for token in tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3711ac31-8d40-4435-b4a8-a7a55bdc6c72",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending on the document title (topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "aa4a4590-8f41-4492-8fef-c1c027ed6ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_from_title(document, nasari):\n",
    "    \"\"\"\n",
    "    :param document: input document\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: a list of Nasari vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    title = document[0] #il titolo e' la prima riga del documento\n",
    "    \n",
    "    # topic calculated with BOW approach\n",
    "    topic = bag_of_word_approach(title)\n",
    "\n",
    "    # NB nasari_vectors is a dict of dicts {word: {{term:score},...}}\n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in topic: #per ogni parola del topic\n",
    "        if word in nasari.keys(): #se la parola e' presente nel dizionario nasari\n",
    "            nasari_vectors.append(nasari[word]) #aggiungo al vettore nasari la corrispondente  parola\n",
    "\n",
    "    return nasari_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24591d8e-d92d-468d-93fa-7844e531065a",
   "metadata": {},
   "source": [
    "Create a list of nasari vectors depending of text's terms. Very similar to the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e2769748-b5fb-4959-a9f9-52a4f0cd5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo il vettore NASARI dal testo. Stesso approccio di prima solo che qui e' con il testo e non con il titolo \n",
    "def text_to_nasari(text, nasari):\n",
    "    \"\"\"\n",
    "    :param text: the list of text's terms\n",
    "    :param nasari: Nasari dictionary\n",
    "    :return: list of Nasari's vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = bag_of_word_approach(text)\n",
    "    \n",
    "    nasari_vectors = []\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in nasari.keys():\n",
    "            nasari_vectors.append(nasari[word]) \n",
    "        \n",
    "    return nasari_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb390aff-d337-4e5f-8335-e1b3086fda61",
   "metadata": {},
   "source": [
    "Given a list of paragraph from a document, calculate how many of these are preserved depending to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "43a5084f-feb9-49c5-8c11-d69688207bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vedo i paragrafi da mantenere a seguito della compressione \n",
    "def calculate_lines_to_keep(doc_paragraphs, percentage):\n",
    "\n",
    "    return len(doc_paragraphs) - int(round((percentage / 100) * len(doc_paragraphs), 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fc2a0-1860-4a19-94d0-3d3918d7a50e",
   "metadata": {},
   "source": [
    "Given a list of paragraphs annotated with overlap scores, compute the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "27c392e1-344e-40ea-a3f1-0aac863fa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Riduco il documento in base al numero di paragrafi da mantenere\n",
    "def reduce_document(doc_paragraphs_overlaps, lines_to_keep):\n",
    "    \"\"\"\n",
    "    :param doc_paragraphs_overlaps: document's paragraphs as a list with an overlap score\n",
    "    :param lines_to_keep: number of paragraph to keep\n",
    "    :return: reduced document\n",
    "    \"\"\"\n",
    "    # Order by weighted overlap\n",
    "    document_sorted  = sorted(doc_paragraphs_overlaps, key=lambda x: x[1], reverse=True)\n",
    "    reduced_document = document_sorted[:lines_to_keep] #il documento ridotto e' costituito dall'ordinamento dei paragrafi con overlap maggiore\n",
    "\n",
    "    #print(reduced_document)\n",
    "    \n",
    "    reduced_document = sorted(reduced_document, key=lambda x: x[0], reverse=True) #ordino i paragrafi sulla base del documento ridotto\n",
    "\n",
    "    # Obtain the text\n",
    "    reduced_document_text = list(map(lambda x: x[2], reduced_document)) \n",
    "    \n",
    "    # Add the title\n",
    "    reduced_document_text = [document[0]] + reduced_document_text\n",
    "\n",
    "    #print(reduced_document_text)\n",
    "    \n",
    "    return reduced_document_text\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea8bdc07-454e-4868-9e28-39f84435602e",
   "metadata": {},
   "source": [
    "Applico la Summarization ai documenti applicando il metodo basato sul titolo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "57c88a73-1963-4a61-ab85-b40a2011255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(document, nasari, percentage):\n",
    "\n",
    "    # Obtain the topics from the title\n",
    "    topics = get_topic_from_title(document, nasari)\n",
    "    doc_paragraphs = []\n",
    "    i = 0\n",
    "\n",
    "    # For each paragraph in the document\n",
    "    for doc_paragraph in document[1:]:\n",
    "        \n",
    "        # obtain nasari rappresentation of the paragraph. Quindi per ogni paragrafo del documento ottengo il vettore nasari\n",
    "        nasari_text_par = text_to_nasari(doc_paragraph, nasari)\n",
    "\n",
    "        paragraph_weighted_overlap = 0 #overlap del paragrafo corrente\n",
    "        \n",
    "        #Per ogni parola del paragrafo\n",
    "        # word is a nasari rappresentation of the term {word: {{term:score},...}}\n",
    "        for word in nasari_text_par:\n",
    "            topic_weighted_overlap = 0 #overlap del topic corrente\n",
    "        \n",
    "            for topic in topics:\n",
    "                # for each topic compute the WO for topic and word (comulative)\n",
    "                topic_weighted_overlap += overlap(word, topic)\n",
    "            \n",
    "            # Mean of WO (based on number of topic)\n",
    "            if topic_weighted_overlap != 0:\n",
    "                topic_weighted_overlap /= len(topics) #divido per il numero di topics, per vederne la rilveanza\n",
    "            \n",
    "            # Comulative paragraph's WO\n",
    "            paragraph_weighted_overlap += topic_weighted_overlap\n",
    "\n",
    "        if len(nasari_text_par) != 0:\n",
    "            # Mean of paragraph's WO\n",
    "            paragraph_weighted_overlap /= len(nasari_text_par)\n",
    "            # Create a tuple with paragraph's number, WO and text. Append it.\n",
    "            doc_paragraphs.append((i, paragraph_weighted_overlap, doc_paragraph))\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Obtain number of lines to keep\n",
    "    lines_to_keep = calculate_lines_to_keep(doc_paragraphs, percentage)\n",
    "    \n",
    "    # Finally we can execute summarization\n",
    "    reduced_document = reduce_document(doc_paragraphs, lines_to_keep)\n",
    "    \n",
    "    return reduced_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5620c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(text_summ, path):\n",
    "        with open(path,\"w\") as f:\n",
    "            for paragraph in text_summ:\n",
    "                f.write(paragraph)\n",
    "                f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ae58c-fa26-4b3d-a760-8e3412f3cc8e",
   "metadata": {},
   "source": [
    "Call all previously defined methods.\n",
    "Also calcule BLEU and ROUGE score to see how similar the results are compared to the original documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "8bad6ba3-4a29-4576-b6ac-f57eb7577199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************** Andy-Warhol.txt*******************************\n",
      "10 % reduction\n",
      "BLEU score:  0.8948393168143697\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8813559322033898, 'p': 1.0, 'f': 0.9369369319568218}, 'rouge-2': {'r': 0.8511415525114155, 'p': 0.982086406743941, 'f': 0.9119373727163127}, 'rouge-l': {'r': 0.8813559322033898, 'p': 1.0, 'f': 0.9369369319568218}}]\n",
      "20 % reduction\n",
      "BLEU score:  0.7788007830714049\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7812018489984591, 'p': 1.0, 'f': 0.8771626248332306}, 'rouge-2': {'r': 0.7296803652968037, 'p': 0.9815724815724816, 'f': 0.837087475464543}, 'rouge-l': {'r': 0.7812018489984591, 'p': 1.0, 'f': 0.8771626248332306}}]\n",
      "30 % reduction\n",
      "BLEU score:  0.6514390575310556\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.724191063174114, 'p': 1.0, 'f': 0.8400357413299089}, 'rouge-2': {'r': 0.6621004566210046, 'p': 0.9823848238482384, 'f': 0.7910529139021558}, 'rouge-l': {'r': 0.724191063174114, 'p': 1.0, 'f': 0.8400357413299089}}]\n",
      "\n",
      "\n",
      "**************************** Ebola-virus-disease.txt*******************************\n",
      "10 % reduction\n",
      "BLEU score:  0.8046150583253528\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9588100686498856, 'p': 1.0, 'f': 0.9789719576190334}, 'rouge-2': {'r': 0.943013698630137, 'p': 0.9885123492245835, 'f': 0.9652271402635708}, 'rouge-l': {'r': 0.9588100686498856, 'p': 1.0, 'f': 0.9789719576190334}}]\n",
      "20 % reduction\n",
      "BLEU score:  0.6703200460356393\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9130434782608695, 'p': 1.0, 'f': 0.9545454495557851}, 'rouge-2': {'r': 0.873972602739726, 'p': 0.989454094292804, 'f': 0.9281349964739601}, 'rouge-l': {'r': 0.9130434782608695, 'p': 1.0, 'f': 0.9545454495557851}}]\n",
      "30 % reduction\n",
      "BLEU score:  0.5737534207374327\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7871853546910755, 'p': 1.0, 'f': 0.8809218900773}, 'rouge-2': {'r': 0.749041095890411, 'p': 0.9884309472161966, 'f': 0.8522443841223489}, 'rouge-l': {'r': 0.7871853546910755, 'p': 1.0, 'f': 0.8809218900773}}]\n",
      "\n",
      "\n",
      "**************************** Life-indoors.txt*******************************\n",
      "10 % reduction\n",
      "BLEU score:  0.9200444146293233\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9548192771084337, 'p': 1.0, 'f': 0.9768875142630716}, 'rouge-2': {'r': 0.9324577861163227, 'p': 0.9822134387351779, 'f': 0.9566891191612207}, 'rouge-l': {'r': 0.9548192771084337, 'p': 1.0, 'f': 0.9768875142630716}}]\n",
      "20 % reduction\n",
      "BLEU score:  0.8337529180751805\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8885542168674698, 'p': 1.0, 'f': 0.9409888307430895}, 'rouge-2': {'r': 0.8592870544090057, 'p': 0.9807280513918629, 'f': 0.91599999502178}, 'rouge-l': {'r': 0.8885542168674698, 'p': 1.0, 'f': 0.9409888307430895}}]\n",
      "30 % reduction\n",
      "BLEU score:  0.6411803884299546\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7259036144578314, 'p': 1.0, 'f': 0.8411867316008029}, 'rouge-2': {'r': 0.6679174484052532, 'p': 0.9807162534435262, 'f': 0.7946428523228486}, 'rouge-l': {'r': 0.7259036144578314, 'p': 1.0, 'f': 0.8411867316008029}}]\n",
      "\n",
      "\n",
      "**************************** Napoleon-wiki.txt*******************************\n",
      "10 % reduction\n",
      "BLEU score:  0.8824969025845955\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8536585365853658, 'p': 0.997624703087886, 'f': 0.920043806640314}, 'rouge-2': {'r': 0.8507295173961841, 'p': 0.9831387808041504, 'f': 0.912154026313671}, 'rouge-l': {'r': 0.8536585365853658, 'p': 0.997624703087886, 'f': 0.920043806640314}}]\n",
      "20 % reduction\n",
      "BLEU score:  0.8187307530779819\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.7865853658536586, 'p': 0.9974226804123711, 'f': 0.8795454496152894}, 'rouge-2': {'r': 0.7598204264870931, 'p': 0.9825834542815675, 'f': 0.8569620203981814}, 'rouge-l': {'r': 0.7865853658536586, 'p': 0.9974226804123711, 'f': 0.8795454496152894}}]\n",
      "30 % reduction\n",
      "BLEU score:  0.6807123983233854\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.6788617886178862, 'p': 0.9970149253731343, 'f': 0.8077388101741556}, 'rouge-2': {'r': 0.6318742985409652, 'p': 0.9825479930191972, 'f': 0.769125678296017}, 'rouge-l': {'r': 0.6788617886178862, 'p': 0.9970149253731343, 'f': 0.8077388101741556}}]\n",
      "\n",
      "\n",
      "**************************** Trump-wall.txt*******************************\n",
      "10 % reduction\n",
      "BLEU score:  0.7903383629814982\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.9522409992652462, 'p': 1.0, 'f': 0.9755363141599364}, 'rouge-2': {'r': 0.9195930423367247, 'p': 0.9855786141399929, 'f': 0.9514431189448774}, 'rouge-l': {'r': 0.9485672299779574, 'p': 0.996141975308642, 'f': 0.9717726709533124}}]\n",
      "20 % reduction\n",
      "BLEU score:  0.6910347152078063\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.8339456282145481, 'p': 1.0, 'f': 0.9094551232461201}, 'rouge-2': {'r': 0.7981621266819823, 'p': 0.9842169162282477, 'f': 0.8814787917199404}, 'rouge-l': {'r': 0.8302718589272594, 'p': 0.9955947136563876, 'f': 0.9054487129897099}}]\n",
      "30 % reduction\n",
      "BLEU score:  0.5627048688069557\n",
      "Rogue scores:  [{'rouge-1': {'r': 0.6943423952975754, 'p': 1.0, 'f': 0.8196010359259454}, 'rouge-2': {'r': 0.6334099113882508, 'p': 0.9826883910386965, 'f': 0.7703053235113381}, 'rouge-l': {'r': 0.6914033798677444, 'p': 0.9957671957671957, 'f': 0.816131825171392}}]\n",
      "\n",
      "\n",
      "\n",
      "write completed\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "docs = ['Andy-Warhol.txt', 'Ebola-virus-disease.txt', 'Life-indoors.txt', 'Napoleon-wiki.txt', 'Trump-wall.txt']\n",
    "\n",
    "nasari = read_nasari(NASARI)\n",
    "rouge = Rouge()\n",
    "compression = [10,20,30] #percentage_decreas of 10,20,30% \n",
    "\n",
    "for doc in docs : \n",
    "    print('**************************** {}*******************************'.format(doc))\n",
    "    text_path='texts_to_summarize/' +doc\n",
    "    #text_summ_path = './texts_summarized/' +doc\n",
    "\n",
    "    document = read_doc(text_path)\n",
    "    \n",
    "    for i in compression: #percentage_decreas of 10,20,30% utile per calcolarmi i punteggi\n",
    "        summary = summarization(document, nasari, i)\n",
    "\n",
    "        #write_text(summary,text_summ_path,bool)\n",
    "        print(i, \"% reduction\",)\n",
    "        \n",
    "        # Compute BLEU only for 1-gram\n",
    "        print(\"BLEU score: \", sentence_bleu([document], summary, weights=(1, 0, 0, 0)))\n",
    "    \n",
    "        # COmpute rouge scores for unigram, bigram and l-gram. F1, precision and recall.\n",
    "        rouge_scores = rouge.get_scores(' '.join(summary), ' '.join(document))\n",
    "        print(\"Rogue scores: \", rouge_scores)\n",
    "        \n",
    "    print('\\n')\n",
    "print()\n",
    "\n",
    "##print document reducted in file \"texts_summarized\"\n",
    "for doc in docs:\n",
    "    #print('********{}*******'.format(doc))\n",
    "    text_path = text_path='texts_to_summarize/' +doc\n",
    "\n",
    "    document = read_doc(text_path)\n",
    "    for i in compression:\n",
    "        text_summ_path='./texts_summarized/' +str(i)+'_'+doc\n",
    "        summ = summarization(document, nasari, i)\n",
    "        write_text(summ, text_summ_path)\n",
    "\n",
    "print(\"write completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8713ca3b77628a4414cb19afecce4be886be5a2f883480e0c3ed66d57c8d45ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
